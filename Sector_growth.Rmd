---
title: "Time series forecasting"
author: "Sector growth" 
output:
  html_document:
    css: custom-release.css
    df_print: paged
    number_sections: no
    toc: yes
    toc_float: yes
  html_notebook:
    css: custom-release.css
    toc: yes
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
home_dir <- "~/Downloads/Dropbox/Werk/R\ Scripts/"  # "~/Downloads/Dropbox/Werk/R\ Scripts/" "~/R scripts/"   
setwd(paste0(home_dir, "/R_time_series/"))
library(ggraph) 
library(igraph)

source("project.R")
source("load_companies.R")
source("roll_up_nace_tree.R")
source("forecast_functions.R")

# Configuration file
library(yaml)
config <- read_yaml("config.yml")

# Set up project
open_project("R_time_series", home_dir)

# Set analysis variables
var_date_from <- as.Date(config$date_start) # Start date of time series
var_date_to <- as.Date(config$date_end)     # End date of time series
months_forecast <- config$months_forecast   # Number of months in forecast
```

# Introduction

A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Time series forecasting is the use of a model to predict future values based on previously observed values. In this document the time series of the number of companies in the Netherlands is explored and forecasted using several methods. 

First we'll load and prepp the data. Then we'll take a first look at the sector growth. After which, a time series is created and cleaned. The time series will be broken down in it's components, making us understand the time series' make-up and helping us with the time series forecasting. Now finally the actual forecasting is done by applying several forecasting methods which are compared in accuracy. The best forecasting model is then used to do a forecast of a sector's growth for 5 years.

# Loading and prepping data

Some variables that control the input in document's analysis is put in a config file. The used configuration file, config.yml, looks like this:
```{r Show config.yml file, echo = FALSE}
cat(readLines("config.yml"), sep = '\n')
```

The companies data and SBI code hierarchy data is loaded:
```{r Load data, collapse=TRUE}
# Import & transform company data
tbl_companies <- prep_companies(config$process_companies, 
                                var_date_from, 
                                var_date_to)
# Import branche hierarchy data
tbl_nace <- read.csv2(paste0(dir_input, "/branche_hierarchy.csv"), 
                      stringsAsFactors = FALSE)
```

## Rolling up the sector hierarchy

The number of companies per nace code in the company file can be small, making it unsuitable for time series forecasting. For this reason the number of companies are aggregated though the SBI hierarchy, by propagating codes with the small number of companies to higher hierarchy codes.

To get this mechanism working the first need to add the number of companies for each of the SBI codes to the complete SBI hierarchy.
```{r Add the number of companies to the SBI hierarchy}
# Add number of companies to the NACE hierarchy, so all 
tbl_nace_qty <- tbl_nace %>% 
  left_join(tbl_companies, by = c("code" = "sbi_full")) %>% 
  group_by(code, code_parent, layer_no) %>% 
  summarise(qty = n_distinct(id_giant, na.rm = TRUE)) %>% 
  ungroup() 
```

This is how the SBI hierarchy looks now; many sectors with a small number of companies:
```{r Display NACE hierarchy before roll-up, echo = FALSE, warning = FALSE}
tbl_nodes <- data_frame(code = with(tbl_nace_qty, unique(c(code, code_parent)))) %>% 
  left_join(tbl_nace_qty, by = "code") %>% 
  rename(code_sector = code)
tbl_links <- tbl_nace_qty

tbl_nodes %<>% mutate(layer_no = factor(layer_no))

graph <- graph_from_data_frame(tbl_links, tbl_nodes, directed = TRUE)

# Create Graph
set.seed(42)
ggraph(graph, layout = "nicely") +
  geom_edge_link(edge_width = 0.5, alpha = .4) +
  geom_node_point(aes(colour = layer_no, size = qty), 
                  alpha = 0.4) +
  # geom_node_text(aes(label = code_sector, colour = layer_no), 
  #                repel = TRUE, size = 10) +
  guides(col = FALSE, size = FALSE) +
  scale_color_graydon() +
  theme_graydon("blank")

rm(graph, tbl_nodes, tbl_links)
```

Now the actual roll up is done after which the 'translation table' is joined to the table with companies table:
```{r Rolling up NACE hierarchy}
lst_nace_recoding <- roll_up_nace_tree(tbl_nace_qty, config$qty_companies_in_rolledup)

tbl_companies %<>% 
  left_join(lst_nace_recoding$tbl_dictionary, by = c("sbi_full" = "code"))
```

That results in the hierarchy below, containing far less sectors with a higher number of companies, making forecasting success more likely:
```{r Plot of rotbllled up NACE hierarchy, echo = FALSE, warning = FALSE}
tbl_nace_qty <- tbl_nace %>% 
  left_join(lst_nace_recoding$tbl_rolled_up, by = "code") %>% 
  select(code, code_parent, layer_no = layer_no.x, qty = qty_sticky)

tbl_nodes <- data_frame(code = with(tbl_nace_qty, unique(c(code, code_parent)))) %>% 
  left_join(tbl_nace_qty, by = "code") %>% 
  rename(code_sector = code)
tbl_links <- tbl_nace_qty

tbl_nodes %<>% mutate(layer_no = factor(layer_no))

graph <- graph_from_data_frame(tbl_links, tbl_nodes, directed = TRUE)

# Create Graph
set.seed(42)
ggraph(graph, layout = "nicely") +
  geom_edge_link(edge_width = 0.5, alpha = .4) +
  geom_node_point(aes(colour = layer_no, size = qty), 
                  alpha = 0.4) +
  # geom_node_text(aes(label = code_sector, colour = layer_no), 
  #                repel = TRUE, size = 10) +
  guides(col = FALSE, size = FALSE) +
  scale_color_graydon() +
  theme_graydon("blank")
```
```{r Clean up stuff, echo=FALSE}
rm(graph, tbl_nodes, tbl_links)
```

## Time warping companies

A time series is a series of data points indexed in time order. In  this step the companies are put in each month within a time frame, specified by the config file, according to their date of establishment and discontinuation.

Depending on the processing configuration, the data is processed *or* pre-processed is loaded; after which NACE descriptions are added.
```{r Aggregate the number of companies and time-spread}
# Depending on the processing configuration the data is processed
if(config$process_aggregate){
  tbl_companies_aggr <- aggregate_companies(tbl_companies, lst_nace_recoding$tbl_dictionary)
  saveRDS(tbl_companies_aggr, file = paste0(dir_input, "/companies_aggr.RDS"))
} else {
  # or pre-processed is loaded
  tbl_companies_aggr <- read_rds(paste0(dir_input, "/companies_aggr.RDS"))
}

tbl_companies_aggr %<>%
  rename(code = code_new) %>% 
  left_join(tbl_nace, by = "code")
```

## Initial exploration

### Development of sectors

Now we can take a quick look at the development number of companies in each sector:
```{r Plot number of companies per NACE code by month, fig.width=12, fig.height=10}
ggplot(tbl_companies_aggr, aes(x = month, y = qty_companies)) +
  geom_area(col = col_graydon[1], fill = col_graydon[1], alpha = .6) +
  facet_wrap(~ code) +
  scale_y_continuous(labels = format_number) +
  labs(x = "", y = "# Companies") +
  theme_graydon("grid")
```
```{r echo=FALSE}
var_sector <- (tbl_nace %>% filter(code == config$code_nace))$description
```
The sector with SBI code `r config$code_nace` (`r var_sector`) looks like an interesting candidate, because it has no obvious trend, which would make it more challenging to forecast. 

### Development of the retail sector

When zooming in on this NACE code we see how interesting this is in terms of rises and drops in number of companies:
```{r pressure, echo=FALSE}
tbl_companies_aggr %>% 
  filter(code == config$code_nace) %>% 
ggplot(aes(x = month, y = qty_companies)) +
  geom_line(col = col_graydon[1], alpha = .6) +
  facet_wrap(~ description) +
  scale_y_continuous(labels = format_number) +
  labs(x = "", y = "# Companies") +
  theme_graydon("grid")
```

# Creating the time series

The library **forecast** contains all kinds of time serie goodies and forecasting functions. The **ggfortify** library provides some stuff for plotting and converting time series.
```{r Loading time series packages}
library(forecast)
library(ggfortify)
```

First, the data of the sector '`r var_sector`' we want to explore and forecast is set apart:
```{r Getting specific nace code}
tbl_companies_code <- tbl_companies_aggr %>% filter(code == config$code_nace)
var_sector <- (tbl_nace %>% filter(code == config$code_nace))$description
```

Then, a *ts* object is created from this data using the *ts()* function. The *ts* object is created by :

* a vector with the quantities per month (in this case), 
* the *frequency* i.e. number of units per year (in this case 12, for months) and 
* the date from which the time series should start

```{r Create time series}
ts_companies <- ts(tbl_companies_code$qty_companies, 
                   frequency = 12, 
                   start = c(year(var_date_from), month(var_date_from)) )
```

## Cleaning the time series

The time series is cleaned by using the *ts_clean* function. This function will remove outliers and replace missing values.
```{r Cleaning time series}
ts_companies_clean <- tsclean(ts_companies)
```

When putting the original time series next to the cleaned one you can see the differences:
```{r Compare original and cleaned time series}
plot_time_series(list(ts_companies_clean, ts_companies),
                 c("Read", "Cleaned"))
```

# Time series breakdown

We can see a variety of patterns in time series; for example we can see some larger shifts, regular looking recurring variations in smaller time windows. To understand and or model a time series, it is often helpful to split a time series into several components, each of which represents an underlying pattern category; this process is called decomposition. These categories are:

* Level - height of the time series.
* Trend - Upward or downward moving mean.
* Seasonality - Variations at specific regular intervals less than a year (e.g.: quarterly). If it does not vary with the level: additive, if it does: multiplicative.
* Cycles - Rises and falls without fixed period, typically longer than seasonality and higher variability
* Stationary - Constant mean (i.e. no trend), no changes in variability and frequency. So: no trend, no seasonality and no cycles.

## Determine seasonality

We can use a seasonality plot to see whether there is a systemtic skew of timeseries values in certain systemic parts of the year. A subset of years is taken the plot doesn't get overcrowded. It seems all years are almost circular; so there seems no seasonality.
```{r Plot seasonality}
ts_companies_subset <- window(ts_companies_clean, start = 2012, end = 2017)

ggseasonplot(ts_companies_subset, polar = TRUE, main ="") +
  scale_color_graydon() +
  theme_graydon("grid")
```

## STL decomposition

STL is an acronym for Seasonal and Trend decomposition using Loess; Loess is a method for estimating nonlinear relationships. The method breaks down a time series in a trend-cycle component (shorthandidly called 'trend' here) and a seasonal component. The 'remainder' component is what is left after the trend and seasonal components are removed.
```{r}
p_stl <- decompose_ts(ts_companies_clean)
p_stl$p_stl
```
Note how the scales of each of the componens differ, this is because each graph is additive: add each component and you would get the original time series values.

In this case the remainder does not appear to be stationary since there is variations in spikes. It seems there is something more going on in this time series.

# Modelling forecasts

There are several methods for forecasting time series. The broad categories that are explored here are:

* Naive forecasting models. This is a set of forecasting models, that are at best bad models, but they can serve as a baseline to compare the accuracy of other models. 
* Exponential smoothing forecast models - These use weighted averages of past observations, with the weights decaying exponentially as the observations get older; the more recent the observation, the higher the associated weight. These generate reliable forecasts quickly and for a wide range of time series, which is a great advantage to applications in business.
* ARIMA models - ARIMA stands for Autoregressive Integrated Moving Average models, which is a forecasting technique is a form of regression analysis that seeks to predict future movements along the seemingly random walk. Here only the auto ARIMA model is explained.

## Create a training set

To verify how well the forecast models perform we build the models on a subset of the time series that stops months before the actual time series. The forecast that is created can afterwards be compared to the original time series. We create the subset with... the *subset()* function:
```{r Create a training set}
ts_companies_train <- subset(ts_companies_clean, 
                             end = length(ts_companies_clean) - months_forecast)
```

## Naive forecasting

### Mean forecasting

The mean forecast model is kind of stupid: make the mean of the total time series the forecast for all future time points.
```{r Forecast model - mean}
fit_mean <- meanf(ts_companies_train, h = months_forecast)
```

### Naive forecasting

Naive forecasting is a technique in which the last period's actuals are used as this period's forecast, without adjusting them or attempting to establish causal factors. It is used only for comparison with the forecasts generated by the better techniques. In R we use the *naive* function. There is also a seasonal version of the naive forecast where the forecasts equal to last value from same season; for this the *snaive* function is used
```{r Forecast model - naive}
fit_naive <- naive(ts_companies_train, h = months_forecast)
fit_snaive <- snaive(ts_companies_train, h = months_forecast)
```

### Drift forecasting

Random walk with drift forecasting, the forecasts equal to last value plus average change. This is done with the *rwf* function.
```{r Forecast model - drift}
fit_drift <- rwf(ts_companies_train, drift = TRUE, h = months_forecast) 
```

## Exponential smoothing

### Single Exponential Smoothing

The Single Exponential Smoothing forecast is the old one plus an adjustment for the error that occurred in the last forecast. This method is suitable for forecasting data with no clear trend or seasonal pattern. This is done with the *ses* function.
```{r Forecast model - SES}
fit_ses <- ses(ts_companies_train, h = months_forecast)
```

### Holt’s linear trend forecasting

Holt’s linear trend forecasting extends Simple Exponential Smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend). This is done with the *holt* function.
```{r Forecast model - Holt’s linear trend}
fit_holt <- holt(ts_companies_train, h = months_forecast) 
```

### Holt-Winters’ seasonal forecasting

Holt-Winters’ seasonal forecasting extended Holt’s method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations — one for the level, one for the trend and one for the seasonal component. There are two variations to this method that differ in the nature of the seasonal component. The **additive** method is preferred when the seasonal variations are roughly constant through the series, while the **multiplicative** method is preferred when the seasonal variations are changing proportional to the level of the series.
```{r Forecast model - Holt-Winters’ seasonal}
fit_hw_m <- hw(ts_companies_train, h = months_forecast, seasonal = "multiplicative")
fit_hw_a <- hw(ts_companies_train, h = months_forecast, seasonal = "additive")
```

### Exponential Smoothing State Space Model

So far we've seen:

* (N,N)	=	simple exponential smoothing
* (A,N)	=	Holts linear method
* (M,N)	=	Exponential trend method
* (Ad,N)	=	additive damped trend method
* (Md,N)	=	multiplicative damped trend method
* (A,A)	=	additive Holt-Winters method
* (A,M)	=	multiplicative Holt-Winters method
* (Ad,M)	=	Holt-Winters damped method

                                  Seasonal Component

| Trend Component     | N (None)	| A (Additive)	| M (Multiplicative)|
| :-----              | :---:	| :------:  	|  :---:  |
|N  (None)            | (N,N) | (N,A)	      | (N,M)     |
|A  (Additive)        | (A,N)	| (A,A)	      | (A,M)     |
|Ad (Additive damped) |	(Ad,N)|	(Ad,A)	    | (Ad ,M)   |
|M (Multiplicative)   | (M,N) |	(M,A)	      | (M,M)     | 
|Md (Multiplicative damped) | (Md,N) | (Md,A)  | (Md,M) |

An alternative to estimating the parameters by minimizing the sum of squared errors is to maximize the “likelihood”. The likelihood is the probability of the data arising from the specified model. Thus, a large likelihood is associated with a good model. 
```{r Forecast model - ets}
model_ets <- ets(ts_companies_train)
fit_ets <- forecast(model_ets, h = months_forecast)
```

## ARIMA

### Auto ARIMA

```{r Forecast model - ses}
model_auto.arima <- auto.arima(ts_companies_train)
fit_auto.arima <- forecast(model_auto.arima, h = months_forecast)
```


# Evaluating models

Forecasting models is done by the *evaluate_forecasts()* function, from the *forecast_functions.R*. This function can take a list of all the fitted models and the complete time series *ts_companies_clean*. The function returns a list with:

* a plot ,with the forecasted data points with their confidence intervals and the actual dataset, 
* a data frame with the error measurements of the forecast test sets and
* a plot of with the performance of the models for each accuracy measurement type.

```{r Evaluate forecast models}
lst_fitted <- list(fit_mean, fit_naive, fit_snaive, fit_drift, fit_holt, fit_hw_m, fit_hw_a, 
                   fit_ets, fit_ses, fit_auto.arima)

lst_evaluations <- evaluate_forecasts(lst_fitted, ts_companies_clean)
```


```{r Remove fitted models, echo=FALSE}
rm(fit_mean, fit_naive, fit_snaive, fit_drift, fit_holt, fit_hw_m, fit_hw_a, 
   fit_ets, fit_ses, fit_auto.arima)
```

### All forecast model results

For all models the time series and their forecasted data points with associated confidence intervals of 95% and 80% are plotted. To achieve this, a list of the plots created by retrieving them from the *lst_evaluations* items, of which the plot are the first subitem. Then this list of plots are put into a *grid.arrange()* function, which arranges multiple ggplots in one grid which is displayed as one plot. 
```{r Plot all forecast methods, fig.width=12, fig.height=15, warning=FALSE}
lst_forecast_plots <- sapply(lst_evaluations$lst_plot_forecast, "[", 1) 
do.call("grid.arrange", c(lst_forecast_plots, ncol=3))
```

### Compare models based on MASE evaluation

There are many ways in which to meaure the model's accuracy. The measures which were determined here are:

* ME: Mean Error
* RMSE: Root Mean Squared Error
* MAE: Mean Absolute Error
* MPE: Mean Percentage Error
* MAPE: Mean Absolute Percentage Error
* MASE: Mean Absolute Scaled Error
* ACF1: Autocorrelation of errors at lag 1
* Theil's U: accuracy measure that emphasises the importance of large errors (as in MSE) as well as providing a relative basis for comparison with naïve forecasting methods.

```{r Table with the accuracy measurements for each of the models, echo=FALSE, eval=FALSE}
lst_evaluations$tbl_accuracy %>% 
  group_by(method, measure) %>% 
  summarise(value = max(value)) %>% 
  ungroup() %>% 
  rename(Measure = measure) %>% 
  spread(method, value) %>% 
  knitr::kable()
```

In the plot of each accuracy measurement the best forecasting method is highlighted.
```{r Plots with the model evaluations per accuracy measure, fig.width=12, fig.height=20}
lst_evaluations$p_accuracy
```

I prefer the MASE accuracy indicator because it can be applied to time series with negative values (which is less important here), and does not suffer from problems (other accuracy indicators suffer from: https://en.wikipedia.org/wiki/Mean_absolute_scaled_error.

Most accuracy measures seem to agree that the Holt-Winter's additive method is the best forecasting method.

### The best performing forecast method

Now it's established Holt-Winter's additive method performs best, we will take another look at it's performance:
```{r Plot of the best performing forecast method, warning=FALSE}
no_best <- (lst_evaluations$tbl_accuracy %>%
              filter(measure == "MASE") %>% 
              mutate(row_no = row_number()) %>% 
              mutate(is_best = value == min(value)) %>% 
              filter(is_best))$row_no 
lst_forecast_plots[no_best]$p_forecast
```

# Applying the model

Now the best forecasting method is found for this time series, it is applied for a longer time period (5 years).
```{r Use best method forecast for longer period, warning=FALSE}
fit_hw_a <- hw(ts_companies_train, h = 60, seasonal = "additive")
result_fit <- evaluate_forecast(fit_hw_a, ts_companies_clean)
result_fit$p_forecast
```

