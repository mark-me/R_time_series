---
title: "Sector growth"
author: "Time series forecasting" 
output:
  html_document:
    css: custom-release.css
    df_print: paged
    number_sections: no
    toc: yes
    toc_float: yes
  html_notebook:
    css: custom-release.css
    toc: yes
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
home_dir <- "~/Downloads/Dropbox/Werk/R\ Scripts/"  # "~/Downloads/Dropbox/Werk/R\ Scripts/" "~/R scripts/"   
setwd(paste0(home_dir, "/R_time_series/"))
library(ggraph) 
library(igraph)

source("project.R")
source("load_companies.R")
source("roll_up_nace_tree.R")
source("forecast_functions.R")

# Configuration file
library(yaml)
config <- read_yaml("config.yml")

# Set up project
open_project("R_time_series", home_dir)

# Set analysis variables
var_date_from <- as.Date(config$date_start) # Start date of time series
var_date_to <- as.Date(config$date_end)     # End date of time series
months_forecast <- config$months_forecast   # Number of months in forecast
```

# Loading and preparing data

The used configuration file, config.yml:
```{r Show config.yml file, echo = FALSE}
cat(readLines("config.yml"), sep = '\n')
```

The company data and SBI code hierarchy is loaded:
```{r Load data, collapse=TRUE}
# Import & transform company data
tbl_companies <- prep_companies(config$process_companies, 
                                var_date_from, 
                                var_date_to)
# Import branche hierarchy data
tbl_nace <- read.csv2(paste0(dir_input, "/branche_hierarchy.csv"), 
                      stringsAsFactors = FALSE)
```

## Rolling up the NACE hierarchy

The number of companies per nace code in the company file can be small, making it unsuitable for time series forecasting. For this reason the number of companies are aggregated though the NACE hierarchy, by propagating codes with the small number of companies to higher hierarchy codes.

To get this mechanism working the first need to add the number of companies for each of the NACE codes to the complete NACE hierarchy.
```{r Add the number of companies to the nace hierarchy}
# Add number of companies to the NACE hierarchy, so all 
tbl_nace_qty <- tbl_nace %>% 
  left_join(tbl_companies, by = c("code" = "sbi_full")) %>% 
  group_by(code, code_parent, layer_no) %>% 
  summarise(qty = n_distinct(id_giant, na.rm = TRUE)) %>% 
  ungroup() 
```

This is how the NACE hierarchy looks now; many sectors with a small number of companies:
```{r Display NACE hierarchy before roll-up, echo = FALSE, warning = FALSE}
tbl_nodes <- data_frame(code = with(tbl_nace_qty, unique(c(code, code_parent)))) %>% 
  left_join(tbl_nace_qty, by = "code") %>% 
  rename(code_sector = code)
tbl_links <- tbl_nace_qty

tbl_nodes %<>% mutate(layer_no = factor(layer_no))

graph <- graph_from_data_frame(tbl_links, tbl_nodes, directed = TRUE)

# Create Graph
set.seed(42)
ggraph(graph, layout = "nicely") +
  geom_edge_link(edge_width = 0.5, alpha = .4) +
  geom_node_point(aes(colour = layer_no, size = qty), 
                  alpha = 0.4) +
  # geom_node_text(aes(label = code_sector, colour = layer_no), 
  #                repel = TRUE, size = 10) +
  guides(col = FALSE, size = FALSE) +
  scale_color_graydon() +
  theme_graydon("blank")

rm(graph, tbl_nodes, tbl_links)
```

Now the actual roll up is done after which the 'translation table' is joined to the table with companies table:
```{r Rolling up NACE hierarchy}
lst_nace_recoding <- roll_up_nace_tree(tbl_nace_qty, config$qty_companies_in_rolledup)

tbl_companies %<>% 
  left_join(lst_nace_recoding$tbl_dictionary, by = c("sbi_full" = "code"))
```

That results in the hierarchy below, containing far less sectors with a higher number of companies, making forecasting success more likely:
```{r Plot of rotbllled up NACE hierarchy, echo = FALSE, warning = FALSE}
tbl_nace_qty <- tbl_nace %>% 
  left_join(lst_nace_recoding$tbl_rolled_up, by = "code") %>% 
  select(code, code_parent, layer_no = layer_no.x, qty = qty_sticky)

tbl_nodes <- data_frame(code = with(tbl_nace_qty, unique(c(code, code_parent)))) %>% 
  left_join(tbl_nace_qty, by = "code") %>% 
  rename(code_sector = code)
tbl_links <- tbl_nace_qty

tbl_nodes %<>% mutate(layer_no = factor(layer_no))

graph <- graph_from_data_frame(tbl_links, tbl_nodes, directed = TRUE)

# Create Graph
set.seed(42)
ggraph(graph, layout = "nicely") +
  geom_edge_link(edge_width = 0.5, alpha = .4) +
  geom_node_point(aes(colour = layer_no, size = qty), 
                  alpha = 0.4) +
  # geom_node_text(aes(label = code_sector, colour = layer_no), 
  #                repel = TRUE, size = 10) +
  guides(col = FALSE, size = FALSE) +
  scale_color_graydon() +
  theme_graydon("blank")
```
```{r Clean up stuff, echo=FALSE}
rm(graph, tbl_nodes, tbl_links)
```

## Spreading companies through time

A time series is a series of data points indexed in time order. In  this step the companies are put in each month within a time frame, specified by the config file, according to their date of establishment and discontinuation.

Depending on the processing configuration, the data is processed *or* pre-processed is loaded; after which NACE descriptions are added.
```{r Aggregate the number of companies and time-spread}
# Depending on the processing configuration the data is processed
if(config$process_aggregate){
  tbl_companies_aggr <- aggregate_companies(tbl_companies, lst_nace_recoding$tbl_dictionary)
  saveRDS(tbl_companies_aggr, file = paste0(dir_input, "/companies_aggr.RDS"))
} else {
  # or pre-processed is loaded
  tbl_companies_aggr <- read_rds(paste0(dir_input, "/companies_aggr.RDS"))
}

tbl_companies_aggr %<>%
  rename(code = code_new) %>% 
  left_join(tbl_nace, by = "code")
```

# Initial exploration

## Development of the number of companies per sector

Now we can take a quick look at the development number of companies in each sector:
```{r Plot number of companies per NACE code by month}
ggplot(tbl_companies_aggr, aes(x = month, y = qty_companies)) +
  geom_area(col = col_graydon[2], fill = col_graydon[2], alpha = .6) +
  facet_wrap(~ code) +
  scale_y_continuous(labels = format_number) +
  labs(x = "", y = "# Companies") +
  theme_graydon("grid")
```
The sector with NACE code `r config$code_nace` looks like an interesting candidate, because it has no obvious trend, which would make it more challenging to forecast. 

## Development of retail sector

When zooming in on this NACE code we see how interesting this is in terms of rises and drops in number of companies:
```{r pressure, echo=FALSE}
tbl_companies_aggr %>% 
  filter(code == config$code_nace) %>% 
ggplot(aes(x = month, y = qty_companies)) +
  geom_area(col = col_graydon[2], fill = col_graydon[2], alpha = .6) +
  facet_wrap(~ description) +
  scale_y_continuous(labels = format_number) +
  labs(x = "", y = "# Companies") +
  theme_graydon("grid")
```

# Creating and cleaning the time series

The library **forecast** contains all kinds of time serie goodies and forecasting functions. The **ggfortify** library provides some stuff for plotting and converting time series.
```{r Loading time series packages}
library(forecast)
library(ggfortify)
```

First, the data of the sector we want to explore and forecast is set apart:
```{r Getting specific nace code}
tbl_companies_code <- tbl_companies_aggr %>% filter(code == config$code_nace)
var_sector <- (tbl_nace %>% filter(code == config$code_nace))$description
```

Then a *ts* object is created from this data, by the *ts()* function. The *ts* object is created by :

* a vector with the quantities per month (in this case), 
* the *frequency* i.e. number of units per year (in this case 12, for months) and 
* the date from which the time series should start

```{r Create time series}
ts_companies <- ts(tbl_companies_code$qty_companies, 
                   frequency = 12, 
                   start = c(year(var_date_from), month(var_date_from)) )
```

## Cleaning the time series

The time series is cleaned by using the *ts_clean* function. This function will remove outliers and replace missing values.
```{r Cleaning time series}
ts_companies_clean <- tsclean(ts_companies)
```

When putting the original time series next to the cleaned one you can see the differences:
```{r Compare original and cleaned time series}
plot_time_series(list(ts_companies, ts_companies_clean),
                 c("Read", "Cleaned"))
```

## Time series decomposition

Thus we think of a time series as comprising three components: a trend-cycle component, a seasonal component, and a remainder component (containing anything else in the time series).

## Determine seasonality

We can use a seasonality plot to see whether there is a systemtic skew of timeseries values in certain systemic parts of the year. A subset of years is taken the plot doesn't get overcrowded. It seems all years are almost circular; so there seems no seasonality.
```{r Plot seasonality}
ts_companies_subset <- window(ts_companies_clean, start = 2012, end = 2017)

ggseasonplot(ts_companies_subset, polar = TRUE, main ="") +
  scale_color_graydon() +
  theme_graydon("grid")

rm(ts_companies_subset)
```

# Forecasting

## Create a training set
```{r Create a training set}
ts_companies_train <- subset(ts_companies_clean, end = length(ts_companies_clean) - months_forecast)
```

## Forecasting modeling techniques

Three kinds of models are tested here:

* Exponential smoothing forecast models
    - Use weighted averages of past observations, with the weights decaying exponentially as the observations get older; the more recent the observation, the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in business.
* ARIMA models
  * Auto ARIMA model

### Mean forecasting

The mean forecast model is kind of stupid: make the mean of the total time series the forecast for all future time points.
```{r Forecast model - mean}
fit_mean <- meanf(ts_companies_train, h = months_forecast)
```

### Naive forecasting

Naive forecasting is a technique in which the last period's actuals are used as this period's forecast, without adjusting them or attempting to establish causal factors. It is used only for comparison with the forecasts generated by the better techniques. In R we use the *naive* function. There is also a seasonal version of the naive forecast where the forecasts equal to last value from same season; for this the *snaive* function is used
```{r Forecast model - naive}
fit_naive <- naive(ts_companies_train, h = months_forecast)
fit_snaive <- snaive(ts_companies_train, h = months_forecast)
```

### Drift forecasting

Random walk with drift forecasting, the forecasts equal to last value plus average change. This is done with the *rwf* function.
```{r Forecast model - drift}
fit_drift <- rwf(ts_companies_train, drift = TRUE, h = months_forecast) 
```

### Single Exponential Smoothing

The Single Exponential Smoothing forecast is the old one plus an adjustment for the error that occurred in the last forecast. This method is suitable for forecasting data with no clear trend or seasonal pattern. This is done with the *ses* function.
```{r Forecast model - SES}
fit_ses <- ses(ts_companies_train, h = months_forecast)
```

### Holt’s linear trend forecasting

Holt’s linear trend forecasting extends Simple Exponential Smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend). This is done with the *holt* function.
```{r Forecast model - Holt’s linear trend}
fit_holt <- holt(ts_companies_train, h = months_forecast) 
```

### Holt-Winters’ seasonal forecasting

Holt-Winters’ seasonal forecasting extended Holt’s method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations — one for the level, one for the trend and one for the seasonal component. There are two variations to this method that differ in the nature of the seasonal component. The **additive** method is preferred when the seasonal variations are roughly constant through the series, while the **multiplicative** method is preferred when the seasonal variations are changing proportional to the level of the series.
```{r Forecast model - Holt-Winters’ seasonal}
fit_hw_m <- hw(ts_companies_train, h = months_forecast, seasonal = "multiplicative")
fit_hw_a <- hw(ts_companies_train, h = months_forecast, seasonal = "additive")
```

Seasonal Component

| Trend Component     | N (None)	| A (Additive)	| M (Multiplicative)|
| :-----              | :---:	| :------:  	|  :---:  |
|N  (None)            | (N,N) | (N,A)	      | (N,M)     |
|A  (Additive)        | (A,N)	| (A,A)	      | (A,M)     |
|Ad (Additive damped) |	(Ad,N)|	(Ad,A)	    | (Ad ,M)   |

An alternative to estimating the parameters by minimizing the sum of squared errors is to maximize the “likelihood”. The likelihood is the probability of the data arising from the specified model. Thus, a large likelihood is associated with a good model. 
```{r Forecast model - ets}
model_ets <- ets(ts_companies_train)
fit_ets <- forecast(model_ets, h = months_forecast)
```


```{r Forecast model - ses}
model_auto.arima <- auto.arima(ts_companies_train)
fit_auto.arima <- forecast(model_auto.arima, h = months_forecast)
```

## Evaluating the forecast models

Forecasting models is done by the *evaluate_forecast()* function, from the *forecast_functions.R*, which returns a plot ,with the forecasted data points with their confidence intervals and the actual dataset, and also a data frame with the error measurements of the forecast test set.  

Here the *evaluate_forecast()* is applied to all the fitted models which were put in a list first. The result is put in *lst_evaluation*.
```{r Evaluate forecast models}
lst_fitted <- list(fit_mean, fit_naive, fit_snaive, fit_drift, fit_holt, fit_hw_m, fit_hw_a, 
                   fit_ets, fit_ses, fit_auto.arima)
lst_evaluations <- lapply(lst_fitted, evaluate_forecast, ts_companies_clean) 
```

```{r Remove fitted models, echo=FALSE}
rm(fit_mean, fit_naive, fit_snaive, fit_drift, fit_holt, fit_hw_m, fit_hw_a, 
   fit_ets, fit_ses, fit_auto.arima)
```

### All forecast model results

For all models the time series and their forecasted data points with associated confidence intervals of 95% and 80% are plotted. To achieve this, a list of the plots created by retrieving them from the *lst_evaluations* items, of which the plot are the first subitem. Then this list of plots are put into a *grid.arrange()* function, which arranges multiple ggplots in one grid which is displayed as one plot. 
```{r Plot all forecast methods, warning=FALSE}
lst_forecast_plots <- sapply(lst_evaluations, "[", 1)
do.call("grid.arrange", c(lst_forecast_plots, ncol=3))
```

### Compare models based on MASE evaluation

There are many ways in which to meaure the model's accuracy. The measures which were determined here are:

* ME: Mean Error
* RMSE: Root Mean Squared Error
* MAE: Mean Absolute Error
* MPE: Mean Percentage Error
* MAPE: Mean Absolute Percentage Error
* MASE: Mean Absolute Scaled Error
* ACF1: Autocorrelation of errors at lag 1
* Theil's U: accuracy measure that emphasises the importance of large errors (as in MSE) as well as providing a relative basis for comparison with naïve forecasting methods.

I prefer the MASE accuracy indicator because it can be applied to time series with negative values (which is less important here), and does not suffer from problems (other accuracy indicators suffer from: https://en.wikipedia.org/wiki/Mean_absolute_scaled_error.

The best forecasting method is highlighted automatically here by creating a variable that indicates which row contains the lowest MASE value.
```{r Plot MASE of all forecast models}
tbl_forecast_mase <- do.call("rbind", sapply(lst_evaluations, "[", 2))

tbl_forecast_mase %>% 
  mutate(is_best = MASE == min(MASE)) %>% 
  ggplot(aes(x = reorder(method, MASE), y = MASE)) +
  geom_col(aes(fill = is_best)) +
  geom_text(aes(label = round(MASE, 2)), 
            hjust = -.1) +
  scale_fill_graydon() +
  coord_flip() +
  labs(x = "") +
  guides(fill = FALSE) +
  theme_graydon("vertical")
```
The best forecasting method seems to be Holt-Winter's additive method.

### The best performing forecast method

Now it's established Holt-Winter's additive method performs best, we will take another look at it's performance:
```{r Plot of the best performing forecast method, warning=FALSE}
no_best <- (tbl_forecast_mase %>% 
              mutate(row_no = row_number()) %>% 
              mutate(is_best = MASE == min(MASE)) %>% 
              filter(is_best))$row_no 
lst_forecast_plots[no_best]$p_forecast
```

## Apply best forecasting method

Now the best forecasting method is found for this time series, it is applied for a longer time period (5 years).
```{r Use best method forecast for longer period, warning=FALSE}
fit_hw_a <- hw(ts_companies_train, h = 60, seasonal = "additive")
result_fit <- evaluate_forecast(fit_hw_a, ts_companies_clean)
result_fit$p_forecast
```

